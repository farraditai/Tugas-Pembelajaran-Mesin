{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gdS84cMZTeT"
      },
      "source": [
        "Make your first XGBoost case : https://xgboost.readthedocs.io/en/latest/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAV7sdAYOdNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22194cce-536f-44f3-ee25-38478c99ec48"
      },
      "source": [
        "#predict_leaf_indices\n",
        "\n",
        "import os\n",
        "import xgboost as xgb\n",
        "\n",
        "dtrain = xgb.DMatrix('agaricus.txt.train') #https://github.com/dmlc/xgboost/blob/master/demo/data/agaricus.txt.train\n",
        "dtest = xgb.DMatrix('agaricus.txt.test') #https://github.com/dmlc/xgboost/blob/master/demo/data/agaricus.txt.test\n",
        "\n",
        "watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
        "###\n",
        "# advanced: start from a initial base prediction\n",
        "#\n",
        "print('start running example to start from a initial prediction')\n",
        "# specify parameters via map, definition are same as c++ version\n",
        "param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}\n",
        "# train xgboost for 1 round\n",
        "bst = xgb.train(param, dtrain, 1, watchlist)\n",
        "# Note: we need the margin value instead of transformed prediction in\n",
        "# set_base_margin\n",
        "# do predict with output_margin=True, will always give you margin values\n",
        "# before logistic transformation\n",
        "ptrain = bst.predict(dtrain, output_margin=True)\n",
        "ptest = bst.predict(dtest, output_margin=True)\n",
        "dtrain.set_base_margin(ptrain)\n",
        "dtest.set_base_margin(ptest)\n",
        "\n",
        "print('this is result of running from initial prediction')\n",
        "bst = xgb.train(param, dtrain, 1, watchlist)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15:51:08] 6513x127 matrix with 143286 entries loaded from agaricus.txt.train\n",
            "[15:51:08] 1611x127 matrix with 35442 entries loaded from agaricus.txt.test\n",
            "start running example to start from a initial prediction\n",
            "[0]\teval-error:0.042831\ttrain-error:0.046522\n",
            "this is result of running from initial prediction\n",
            "[0]\teval-error:0.021726\ttrain-error:0.022263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu-9ePG0-vQ0",
        "outputId": "6f229318-64fa-4306-b59d-2d3a34394a0c"
      },
      "source": [
        "import os\n",
        "import xgboost as xgb\n",
        "##\n",
        "#  this script demonstrate how to fit generalized linear model in xgboost\n",
        "#  basically, we are using linear model, instead of tree for our boosters\n",
        "\n",
        "dtrain = xgb.DMatrix('agaricus.txt.train')\n",
        "dtest = xgb.DMatrix('agaricus.txt.test')\n",
        "\n",
        "# change booster to gblinear, so that we are fitting a linear model\n",
        "# alpha is the L1 regularizer\n",
        "# lambda is the L2 regularizer\n",
        "# you can also set lambda_bias which is L2 regularizer on the bias term\n",
        "param = {'objective':'binary:logistic', 'booster':'gblinear',\n",
        "         'alpha': 0.0001, 'lambda': 1}\n",
        "\n",
        "# normally, you do not need to set eta (step_size)\n",
        "# XGBoost uses a parallel coordinate descent algorithm (shotgun),\n",
        "# there could be affection on convergence with parallelization on certain cases\n",
        "# setting eta to be smaller value, e.g 0.5 can make the optimization more stable\n",
        "# param['eta'] = 1\n",
        "\n",
        "##\n",
        "# the rest of settings are the same\n",
        "##\n",
        "watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
        "num_round = 4\n",
        "bst = xgb.train(param, dtrain, num_round, watchlist)\n",
        "preds = bst.predict(dtest)\n",
        "labels = dtest.get_label()\n",
        "print('error=%f' % (sum(1 for i in range(len(preds)) if int(preds[i] > 0.5) != labels[i]) / float(len(preds))))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15:51:09] 6513x127 matrix with 143286 entries loaded from agaricus.txt.train\n",
            "[15:51:09] 1611x127 matrix with 35442 entries loaded from agaricus.txt.test\n",
            "[0]\teval-error:0.114836\ttrain-error:0.10456\n",
            "[1]\teval-error:0.117939\ttrain-error:0.105481\n",
            "[2]\teval-error:0.11856\ttrain-error:0.106249\n",
            "[3]\teval-error:0.120422\ttrain-error:0.106863\n",
            "error=0.120422\n"
          ]
        }
      ]
    }
  ]
}